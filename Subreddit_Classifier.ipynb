{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classifier  \n",
    "  \n",
    "The social media platform Reddit is divided up into different subreddits. Each subreddit is a messageboard devoted to a certain topic. Each post contains a title and some combination of body text, images, videos, or links to external websites. People viewing the content can comment on it. Reddit uses a vote system where everybody can vote on each post or comment, either increasing the score (an \"upvote\") or decreasing the score (a \"downvote\") where comments or posts with higher scores generally being made more visible to anybody viewing the website. Some of the popular subreddit topics can include politics, news, humourous content, or AskReddit, a subreddit where people post questions and other people answer the questions in the comment section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape Reddit content, PRAW (the Python Reddit API Wrapper) was employed. It requires a Reddit developer account which gives credentials needed to access the API. Since I am only pulling content and not posting anything, I am accessing the API in read-only mode which requires fewer credentials than accessing it in read-write mode.  \n",
    "  \n",
    "The cell below handles imports and initializes PRAW with my credentials that are loaded via an external file. In order for this to work for you, you will need your own Reddit API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and setting up the reddit API\n",
    "import pandas as pd\n",
    "import os\n",
    "import praw\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "#Client ID, secret, user agent in that order\n",
    "credentials = list(pd.read_csv(os.getcwd()+'/credentials.csv').columns)\n",
    "\n",
    "reddit = praw.Reddit(client_id=credentials[0],\n",
    "                     client_secret=credentials[1],user_agent=credentials[2])\n",
    "\n",
    "#I want:\n",
    "#Title, text, link (if applicable), top comments, author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary test, the cell below gathers the top 1000 posts (highest score) from six subreddits. These subreddits are:  \n",
    "politics, a subreddit focused on US politics,  \n",
    "AskReddit, a subreddit where people post questions and others answer them in the comment section,  \n",
    "Worldnews, a subreddit focused on international, non-US news,  \n",
    "Funny, a subreddit mostly for memes and joke content,  \n",
    "Gaming, a subreddit dedicated to news or stories about videogames,  \n",
    "Aww, a subreddit dedicated to photos and videos of animals, mostly pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathers top posts from a few subreddits\n",
    "politics = list(reddit.subreddit('politics').top(limit=1000))\n",
    "ask = list(reddit.subreddit('askreddit').top(limit=1000))\n",
    "worldnews = list(reddit.subreddit('worldnews').top(limit=1000))\n",
    "funny = list(reddit.subreddit('funny').top(limit=1000))\n",
    "gaming = list(reddit.subreddit('gaming').top(limit=1000))\n",
    "aww = list(reddit.subreddit('aww').top(limit=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will gather lists of PRAW submission objects which then need to have the relevant data extracted from them.  \n",
    "My general idea is to create a Pandas dataframe containing all of the relevant data. Since PRAW requires each submission be handled individually (I cannot just grab the list, type list.title and get a list of all of the titles), I will be creating empty lists and appending the relevant values to them while looping over my lists of submissions.  \n",
    "  \n",
    "For now, Reddit comments will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing empty lists\n",
    "titles = []\n",
    "is_self = []\n",
    "is_video = []\n",
    "selftext = []\n",
    "author = []\n",
    "created = []\n",
    "num_comments = []\n",
    "score = []\n",
    "subreddit = []\n",
    "for i in range(len(politics)):\n",
    "    titles.append(politics[i].title)\n",
    "    is_self.append(politics[i].is_self)\n",
    "    is_video.append(politics[i].is_video)\n",
    "    selftext.append(politics[i].selftext)\n",
    "    author.append(politics[i].author)\n",
    "    created.append(politics[i].created)\n",
    "    num_comments.append(politics[i].num_comments)\n",
    "    score.append(politics[i].score)\n",
    "    subreddit.append(0)\n",
    "for i in range(len(ask)):\n",
    "    titles.append(ask[i].title)\n",
    "    is_self.append(ask[i].is_self)\n",
    "    is_video.append(ask[i].is_video)\n",
    "    selftext.append(ask[i].selftext)\n",
    "    author.append(ask[i].author)\n",
    "    created.append(ask[i].created)\n",
    "    num_comments.append(ask[i].num_comments)\n",
    "    score.append(ask[i].score)\n",
    "    subreddit.append(1)\n",
    "for i in range(len(worldnews)):\n",
    "    titles.append(worldnews[i].title)\n",
    "    is_self.append(worldnews[i].is_self)\n",
    "    is_video.append(worldnews[i].is_video)\n",
    "    selftext.append(worldnews[i].selftext)\n",
    "    author.append(worldnews[i].author)\n",
    "    created.append(worldnews[i].created)\n",
    "    num_comments.append(worldnews[i].num_comments)\n",
    "    score.append(worldnews[i].score)\n",
    "    subreddit.append(2)\n",
    "for i in range(len(funny)):\n",
    "    titles.append(funny[i].title)\n",
    "    is_self.append(funny[i].is_self)\n",
    "    is_video.append(funny[i].is_video)\n",
    "    selftext.append(funny[i].selftext)\n",
    "    author.append(funny[i].author)\n",
    "    created.append(funny[i].created)\n",
    "    num_comments.append(funny[i].num_comments)\n",
    "    score.append(funny[i].score)\n",
    "    subreddit.append(3)\n",
    "for i in range(len(gaming)):\n",
    "    titles.append(gaming[i].title)\n",
    "    is_self.append(gaming[i].is_self)\n",
    "    is_video.append(gaming[i].is_video)\n",
    "    selftext.append(gaming[i].selftext)\n",
    "    author.append(gaming[i].author)\n",
    "    created.append(gaming[i].created)\n",
    "    num_comments.append(gaming[i].num_comments)\n",
    "    score.append(gaming[i].score)\n",
    "    subreddit.append(4)\n",
    "for i in range(len(aww)):\n",
    "    titles.append(aww[i].title)\n",
    "    is_self.append(aww[i].is_self)\n",
    "    is_video.append(aww[i].is_video)\n",
    "    selftext.append(aww[i].selftext)\n",
    "    author.append(aww[i].author)\n",
    "    created.append(aww[i].created)\n",
    "    num_comments.append(aww[i].num_comments)\n",
    "    score.append(aww[i].score)\n",
    "    subreddit.append(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a Pandas dataframe to hold the data\n",
    "dat = pd.DataFrame()\n",
    "dat['title'] = titles\n",
    "dat['text'] = selftext\n",
    "dat['author'] = author\n",
    "dat['created'] = created\n",
    "dat['score'] = score\n",
    "dat['num_comments'] = num_comments\n",
    "dat['is_self'] = is_self\n",
    "dat['is_video'] = is_video\n",
    "dat['subreddit'] = subreddit\n",
    "dat = dat.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 titles removed\n"
     ]
    }
   ],
   "source": [
    "#Here, short titles are dropped since they are unlikely to have much predictive power \n",
    "from nltk.tokenize import word_tokenize\n",
    "short_count = 0\n",
    "for i in range(len(dat)):\n",
    "    if (len(word_tokenize(dat['title'][i]))<2):\n",
    "        short_count = short_count + 1\n",
    "        dat = dat.drop(i,axis='index')\n",
    "print(str(short_count)+' titles removed')\n",
    "#Resets the index of the dataframe due to the dropped frames\n",
    "dat = dat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is wrangled into a format that is easier to use, I am just going to shove it into the fasttext library Facebook keeps on their research Github. It might not work that well, but it'll at least be a quick prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the fasttext package direct from Facebook's Github will be employed, mostly because using it is pretty simple and does not really require the normal NLP preprocessing.  \n",
    "  \n",
    "It can be trained in supervised mode which supplies a classifier, though it requires the data be loaded from an external text file where each entry looks like this:  \n",
    "  \n",
    "\\__label__*thelabel* The text  \n",
    "  \n",
    "Where \"*thelabel*\" is the label of that data point and \"The text\" is the actual text. The part \"\\__label__\" needs to be  placed as is to mark what the label is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(dat['title'].to_numpy(),open('Titles.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits into training and test data\n",
    "train = dat[0:int(len(dat)*.9)]\n",
    "test = dat[int(len(dat)*.9):len(dat)].reset_index(drop=True)\n",
    "datafile = []\n",
    "#Generates data file for output based on training data\n",
    "for i in range(len(train)):\n",
    "    datafile.append('__label__'+str(train['subreddit'][i])+' '+train['title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This saves the data as a text file as per fasttext's requirements\n",
    "#this opens the data file\n",
    "outfile = open('data.txt','w',errors='ignore')\n",
    "#This runs through the data and adds each line\n",
    "for line in datafile:\n",
    "    outfile.write(line)\n",
    "    outfile.write('\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains the fasttext model\n",
    "model = fasttext.train_supervised('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This runs all of the training and testing examples through the fasttext classifier\n",
    "trainout = []\n",
    "for i in range(len(train)):\n",
    "    trainout.append(int(model.predict(train['title'][i])[0][0][9]))\n",
    "testout = []\n",
    "for i in range(len(test)):\n",
    "    testout.append(int(model.predict(test['title'][i])[0][0][9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is: 0.6305696559503666\n",
      "The test set accuracy is: 0.5996621621621622\n"
     ]
    }
   ],
   "source": [
    "#This counts up the training and test error\n",
    "trainacc = 0\n",
    "#loops over training data\n",
    "for i in range(len(train)):\n",
    "    #If the model's output is different from the \n",
    "    if (train['subreddit'][i]!=trainout[i]):\n",
    "        trainacc = trainacc + 1\n",
    "trainacc = 1- trainacc/len(trainout)\n",
    "\n",
    "testacc = 0\n",
    "for i in range(len(test)):\n",
    "    if (test['subreddit'][i] != testout[i]):\n",
    "        testacc = testacc + 1\n",
    "testacc = 1-testacc/len(testout)\n",
    "print('The training set accuracy is: '+str(trainacc))\n",
    "print('The test set accuracy is: '+str(testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics askreddit worldnews funny gaming aww\n",
      "          0         1         2         3         4         5\n",
      "0  0.728972  0.018692  0.205607  0.046729  0.000000  0.000000\n",
      "1  0.000000  0.968421  0.010526  0.000000  0.021053  0.000000\n",
      "2  0.392157  0.029412  0.549020  0.019608  0.009804  0.000000\n",
      "3  0.009434  0.037736  0.066038  0.415094  0.188679  0.283019\n",
      "4  0.012821  0.051282  0.038462  0.461538  0.294872  0.141026\n",
      "5  0.009615  0.019231  0.028846  0.221154  0.125000  0.596154\n"
     ]
    }
   ],
   "source": [
    "#This uses sklearn to plot the confusion matrix for the test set, just to see how it looks\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('politics','askreddit','worldnews','funny','gaming','aww')\n",
    "confusion = pd.DataFrame(confusion_matrix(test['subreddit'],testout,normalize='true'))\n",
    "print(confusion)\n",
    "#predicted along x-axis, true along y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix actually shows some pretty interesting results. The highest accuracy rating goes to AskReddit where it correctly predicts it 95% of the time while also rarely falsely predicting askreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work very well. Maybe it's the classifier, instead of using the supervised fasttext model, I will try the unsupervised fasttext embeddings and then use boosting. I had a hard time finding a pretrained Fasttext model, so here I train it myself."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gensim.downloader as api\n",
    "dataset = api.load('text8')\n",
    "c = list(dataset)\n",
    "from gensim.models import FastText\n",
    "unsupftt = FastText(size=150)\n",
    "unsupftt.build_vocab(c)\n",
    "\n",
    "#model = fasttext.load_model(os.getcwd()+'/fasttextmodel.vec')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#This model is pickled since training takes quite a while\n",
    "unsupftt.train(c,total_examples=unsupftt.corpus_count,epochs=5)\n",
    "#Saves the model above as a pickle\n",
    "import pickle\n",
    "\n",
    "pickle.dump(unsupftt,open('unsupFT.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#loads the model\n",
    "unsupftt = pickle.load(open('unsupFT.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "embeddings = np.zeros((150,len(dat)))\n",
    "for i in range(len(dat)):\n",
    "    tokenized = word_tokenize(dat['title'][i])\n",
    "    avg = np.zeros((1,150))\n",
    "    for j in range(len(tokenized)):\n",
    "        avg = avg + unsupftt.wv.get_vector(tokenized[j])\n",
    "    avg = avg/len(tokenized)\n",
    "    embeddings[:,i] = avg\n",
    "    \n",
    "embeddings = embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:29:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=3, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xg\n",
    "trainx = embeddings[0:5500]\n",
    "testx = embeddings[5501:]\n",
    "trainy = dat['subreddit'][0:5500]\n",
    "testy = dat['subreddit'][5501:]\n",
    "model = xg.XGBClassifier(gamma=3,subsample=0.8)\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9727272727272728\n",
      "Test set accuracy:\n",
      "0.5195121951219512\n",
      "          0         1         2         3         4         5\n",
      "0  0.693333  0.013333  0.213333  0.000000  0.000000  0.080000\n",
      "1  0.015625  0.656250  0.046875  0.109375  0.125000  0.046875\n",
      "2  0.189189  0.054054  0.675676  0.027027  0.027027  0.027027\n",
      "3  0.102941  0.088235  0.044118  0.264706  0.205882  0.294118\n",
      "4  0.092593  0.111111  0.074074  0.240741  0.333333  0.148148\n",
      "5  0.040000  0.120000  0.026667  0.240000  0.133333  0.440000\n"
     ]
    }
   ],
   "source": [
    "#With the trained model, show accuracy and the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tfidftrain = model.predict(trainx)\n",
    "tfidfout = model.predict(testx)\n",
    "confusion2 = pd.DataFrame(confusion_matrix(testy,tfidfout,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==tfidftrain)/len(tfidftrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==tfidfout)/len(tfidfout))\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "FastText didn't seem to work that well, though the data up until now has barely been cleaned or processed, which is typically important in natural language applications. I start with the following: \n",
    "  \n",
    "- Word Tokenization (splitting the strings into a list of one-word strings)\n",
    "- Removal of stopwords\n",
    "- Lemmatization or stemming\n",
    "- Moving to a vector representation (bag of words, word2vec, or fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "titles = []\n",
    "#This tokenizes, removes stopwords and stems\n",
    "for i in range(len(dat)):\n",
    "    #tokenizes by word\n",
    "    tmp_title = word_tokenize(dat['title'][i].lower())\n",
    "    #removes stop words\n",
    "    tmp_title2 = [i for i in tmp_title if i not in stop]\n",
    "    #stems the words\n",
    "    tmp_title3 = [ps.stem(i) for i in tmp_title2]\n",
    "    titles.append(tmp_title3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn's TF-IDF vectorizer actually seems to do a lot of the work for you, so I'll give that a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=((1,3)))\n",
    "x = vectorizer.fit_transform(dat['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(vectorizer,open('tfidf.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:30:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.3, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xg\n",
    "#Split into training and test data\n",
    "trainx = x[0:5500]\n",
    "testx = x[5501:]\n",
    "trainy = dat['subreddit'][0:5500]\n",
    "testy = dat['subreddit'][5501:]\n",
    "#Instantiate model with given hyperparameters\n",
    "model = xg.XGBClassifier(gamma=0.3,subsample=0.8)\n",
    "#Train model\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9145454545454546\n",
      "Test set accuracy:\n",
      "0.724390243902439\n",
      "          0         1         2         3         4         5\n",
      "0  0.840000  0.026667  0.106667  0.000000  0.026667  0.000000\n",
      "1  0.000000  0.984375  0.015625  0.000000  0.000000  0.000000\n",
      "2  0.135135  0.027027  0.729730  0.040541  0.067568  0.000000\n",
      "3  0.000000  0.014706  0.029412  0.588235  0.117647  0.250000\n",
      "4  0.000000  0.037037  0.092593  0.314815  0.500000  0.055556\n",
      "5  0.000000  0.013333  0.053333  0.226667  0.040000  0.666667\n"
     ]
    }
   ],
   "source": [
    "#With the trained model, show accuracy and the confusion matrix\n",
    "#Get output values\n",
    "tfidftrain = model.predict(trainx)\n",
    "tfidfout = model.predict(testx)\n",
    "#Puts the confusion matrix in a Pandas dataframe so that it prints nicer\n",
    "confusion2 = pd.DataFrame(confusion_matrix(testy,tfidfout,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==tfidftrain)/len(tfidftrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==tfidfout)/len(tfidfout))\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open('tfidfbooster.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1 2]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is a little surprising, tf-idf actually seems to work better than FastText here, both in having a greater overall test set accuracy and in resolving some of the issues with the confusion matrix. Its ability to tell apart the subreddits \"funny\" and \"gaming,\" while still not stellar, has notably improved from FastText. This could be partly because the titles are typically quite short which would make word embeddings lose some of their power relative to a TF-IDF approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.986\n",
      "Test set accuracy:\n",
      "0.6853658536585366\n",
      "          0         1         2         3         4         5\n",
      "0  0.746667  0.053333  0.160000  0.000000  0.013333  0.026667\n",
      "1  0.000000  0.984375  0.000000  0.000000  0.015625  0.000000\n",
      "2  0.121622  0.054054  0.797297  0.013514  0.013514  0.000000\n",
      "3  0.014706  0.191176  0.029412  0.235294  0.117647  0.411765\n",
      "4  0.037037  0.203704  0.037037  0.166667  0.481481  0.074074\n",
      "5  0.013333  0.080000  0.013333  0.053333  0.026667  0.813333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#This data is continuous so Gaussian naive Bayes may make more sense, but sparse data (like output from TF-IDF)\n",
    "#is unlikely to fit a Gaussian distribution, so even though multinomial naive Bayes is more suited to categorical\n",
    "#data it is still likely the better choice.\n",
    "\n",
    "#Create multinomial naive Bayes object\n",
    "MnNB = MultinomialNB(fit_prior=True)\n",
    "#Fit naive Bayes\n",
    "MnNB.fit(trainx,trainy)\n",
    "#Get output from model on test and training sets\n",
    "MnNBtrain = MnNB.predict(trainx)\n",
    "MnNBtest = MnNB.predict(testx)\n",
    "confusion3 = pd.DataFrame(confusion_matrix(testy,MnNBtest,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==MnNBtrain)/len(MnNBtrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==MnNBtest)/len(MnNBtest))\n",
    "print(confusion3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrices for XGBoost vs Naive Bayes, they both have distinct strengths and weaknesses. Naive Bayes seems to do better for the classes that are not 3 and 4, whereas XGBoost does a better job of separating 3 and 4. Somehow combining the two classifiers might work well, but Naive Bayes generally seems biased against predicting 3 and 4, especially 3 which it predicts very infrequently. It also predicts the \"more accurate\" classes a lot more often, with classes 1 and 5 being predicted far more often than they actually appear in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what was seen above, with tf-idf performing better than Fasttext, it seems likely that Fasttext was poorly trained. The model was trained with only 40 dimensions due to constraints on available computing power, a Fasttext model with higher dimensionality may perform better. Typically, 300 dimensions is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-17a4b02cc492>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-17a4b02cc492>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    TIME TO RUN THE BERT PREPROCESSOR\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TIME TO RUN THE BERT PREPROCESSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Another option for word embeddings is BERT, which thankfully comes pretrained as part of Tensorflow. BERT may provide better embeddings than TF-IDF and Fasttext.\n",
    "\n",
    "Note: As of January 2021 when this was written, Tensorflow has a bug on Windows that prevents the loading and usage of BERT preprocessors. As a result, I use WSL (Windows Subsystem for Linux) to run the preprocessor in an Ubuntu environment, then pickle the processed data and load it in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#testing imports\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "#from official.nlp import optimization  # to create AdamW optmizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## BERT STUFF\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#text preprocessor\n",
    "#bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Broken until tensorflow gets updated to fix this\n",
    "preprocess(['hello','I like shoes','what the hell man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "#Loading in BERT Model\n",
    "bertModel = hub.load('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#Loading in the processed data:\n",
    "preprocessedText = pickle.load(open('./preprocessed.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessedSubset = {}\n",
    "\n",
    "subsetSize = 500\n",
    "\n",
    "subsetIndex = int(len(preprocessedText[list(preprocessedText)[0]])/subsetSize)\n",
    "\n",
    "\n",
    "preprocessedSubsets = []\n",
    "for j in range(subsetIndex-1):\n",
    "    tmpSubset = {}\n",
    "    for i in preprocessedText:\n",
    "        tmpSubset[i] = preprocessedText[i][j*subsetSize:(j+1)*subsetSize]\n",
    "    preprocessedSubsets.append(tmpSubset)\n",
    "\n",
    "tmpSubset = {}\n",
    "for i in preprocessedText:\n",
    "        tmpSubset[i] = preprocessedText[i][(j+1)*subsetSize:]\n",
    "preprocessedSubsets.append(tmpSubset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooledOutputs = []\n",
    "for data in preprocessedSubsets:\n",
    "    pooledOutputs.append(bertModel(data)['pooled_output'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pooledOutputs[0]\n",
    "\n",
    "for i in range(1,len(pooledOutputs)):\n",
    "    embeddings = np.concatenate([embeddings,pooledOutputs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dat['subreddit'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = np.array(range(len(labels)))\n",
    "np.random.shuffle(shuffler)\n",
    "\n",
    "embeddingsshuffle = embeddings[shuffler]\n",
    "labelsshuffle = labels[shuffler]\n",
    "\n",
    "trainx = embeddingsshuffle[0:4000]\n",
    "trainy = labelsshuffle[0:4000]\n",
    "testx = embeddingsshuffle[4001:]\n",
    "testy = labelsshuffle[4001:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model with BERT Output Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "#Instantiate model with given hyperparameters\n",
    "model = xg.XGBClassifier(use_label_encoder=False)\n",
    "#Train model\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#Puts the confusion matrix in a Pandas dataframe so that it prints nicer\n",
    "XGBTrainOut = model.predict(trainx)\n",
    "XGBERTOut = model.predict(testx)\n",
    "confusion = pd.DataFrame(confusion_matrix(testy,XGBERTOut,normalize=None))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==XGBTrainOut)/len(XGBTrainOut))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==XGBERTOut)/len(XGBERTOut))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB = GaussianNB()\n",
    "\n",
    "NB.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Puts the confusion matrix in a Pandas dataframe so that it prints nicer\n",
    "trainOut = NB.predict(trainx)\n",
    "testOut = NB.predict(testx)\n",
    "confusion = pd.DataFrame(confusion_matrix(testy,testOut,normalize=None))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==trainOut)/len(trainOut))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==testOut)/len(testOut))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
