{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classifier  \n",
    "  \n",
    "The social media platform Reddit is divided up into different subreddits. Each subreddit is a messageboard devoted to a certain topic. Each post contains a title and some combination of body text, images, videos, or links to external websites. People viewing the content can comment on it. Reddit uses a vote system where everybody can vote on each post or comment, either increasing the score (an \"upvote\") or decreasing the score (a \"downvote\") where comments or posts with higher scores generally being made more visible to anybody viewing the website. Some of the popular subreddit topics can include politics, news, humourous content, or AskReddit, a subreddit where people post questions and other people answer the questions in the comment section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape Reddit content, PRAW (the Python Reddit API Wrapper) was employed. It requires a Reddit developer account which gives credentials needed to access the API. Since I am only pulling content and not posting anything, I am accessing the API in read-only mode which requires fewer credentials than accessing it in read-write mode.  \n",
    "  \n",
    "The cell below handles imports and initializes PRAW with my credentials that are loaded via an external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and setting up the reddit API\n",
    "import pandas as pd\n",
    "import os\n",
    "import praw\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "#Client ID, secret, user agent in that order\n",
    "credentials = list(pd.read_csv(os.getcwd()+'/credentials.csv').columns)\n",
    "\n",
    "reddit = praw.Reddit(client_id=credentials[0],\n",
    "                     client_secret=credentials[1],user_agent=credentials[2])\n",
    "\n",
    "#I want:\n",
    "#Title, text, link (if applicable), top comments, author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary test, the cell below gathers the top 1000 posts (highest score) from six subreddits. These subreddits are:  \n",
    "politics, a subreddit focused on US politics,  \n",
    "AskReddit, a subreddit where people post questions and others answer them in the comment section,  \n",
    "Worldnews, a subreddit focused on international, non-US news,  \n",
    "Funny, a subreddit mostly for memes and joke content,  \n",
    "Gaming, a subreddit dedicated to news or stories about videogames,  \n",
    "Aww, a subreddit dedicated to photos and videos of animals, mostly pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathers top posts from a few subreddits\n",
    "politics = list(reddit.subreddit('politics').top(limit=1000))\n",
    "ask = list(reddit.subreddit('askreddit').top(limit=1000))\n",
    "worldnews = list(reddit.subreddit('worldnews').top(limit=1000))\n",
    "funny = list(reddit.subreddit('funny').top(limit=1000))\n",
    "gaming = list(reddit.subreddit('gaming').top(limit=1000))\n",
    "aww = list(reddit.subreddit('aww').top(limit=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will gather lists of PRAW submission objects which then need to have the relevant data extracted from them.  \n",
    "My general idea is to create a Pandas dataframe containing all of the relevant data. Since PRAW requires each submission be handled individually (I cannot just grab the list, type list.title and get a list of all of the titles), I will be creating empty lists and appending the relevant values to them while looping over my lists of submissions.  \n",
    "  \n",
    "For now, Reddit comments will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing empty lists\n",
    "titles = []\n",
    "is_self = []\n",
    "is_video = []\n",
    "selftext = []\n",
    "author = []\n",
    "created = []\n",
    "num_comments = []\n",
    "score = []\n",
    "subreddit = []\n",
    "for i in range(len(politics)):\n",
    "    titles.append(politics[i].title)\n",
    "    is_self.append(politics[i].is_self)\n",
    "    is_video.append(politics[i].is_video)\n",
    "    selftext.append(politics[i].selftext)\n",
    "    author.append(politics[i].author)\n",
    "    created.append(politics[i].created)\n",
    "    num_comments.append(politics[i].num_comments)\n",
    "    score.append(politics[i].score)\n",
    "    subreddit.append(0)\n",
    "for i in range(len(ask)):\n",
    "    titles.append(ask[i].title)\n",
    "    is_self.append(ask[i].is_self)\n",
    "    is_video.append(ask[i].is_video)\n",
    "    selftext.append(ask[i].selftext)\n",
    "    author.append(ask[i].author)\n",
    "    created.append(ask[i].created)\n",
    "    num_comments.append(ask[i].num_comments)\n",
    "    score.append(ask[i].score)\n",
    "    subreddit.append(1)\n",
    "for i in range(len(worldnews)):\n",
    "    titles.append(worldnews[i].title)\n",
    "    is_self.append(worldnews[i].is_self)\n",
    "    is_video.append(worldnews[i].is_video)\n",
    "    selftext.append(worldnews[i].selftext)\n",
    "    author.append(worldnews[i].author)\n",
    "    created.append(worldnews[i].created)\n",
    "    num_comments.append(worldnews[i].num_comments)\n",
    "    score.append(worldnews[i].score)\n",
    "    subreddit.append(2)\n",
    "for i in range(len(funny)):\n",
    "    titles.append(funny[i].title)\n",
    "    is_self.append(funny[i].is_self)\n",
    "    is_video.append(funny[i].is_video)\n",
    "    selftext.append(funny[i].selftext)\n",
    "    author.append(funny[i].author)\n",
    "    created.append(funny[i].created)\n",
    "    num_comments.append(funny[i].num_comments)\n",
    "    score.append(funny[i].score)\n",
    "    subreddit.append(3)\n",
    "for i in range(len(gaming)):\n",
    "    titles.append(gaming[i].title)\n",
    "    is_self.append(gaming[i].is_self)\n",
    "    is_video.append(gaming[i].is_video)\n",
    "    selftext.append(gaming[i].selftext)\n",
    "    author.append(gaming[i].author)\n",
    "    created.append(gaming[i].created)\n",
    "    num_comments.append(gaming[i].num_comments)\n",
    "    score.append(gaming[i].score)\n",
    "    subreddit.append(4)\n",
    "for i in range(len(aww)):\n",
    "    titles.append(aww[i].title)\n",
    "    is_self.append(aww[i].is_self)\n",
    "    is_video.append(aww[i].is_video)\n",
    "    selftext.append(aww[i].selftext)\n",
    "    author.append(aww[i].author)\n",
    "    created.append(aww[i].created)\n",
    "    num_comments.append(aww[i].num_comments)\n",
    "    score.append(aww[i].score)\n",
    "    subreddit.append(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a Pandas dataframe to hold the data\n",
    "dat = pd.DataFrame()\n",
    "dat['title'] = titles\n",
    "dat['text'] = selftext\n",
    "dat['author'] = author\n",
    "dat['created'] = created\n",
    "dat['score'] = score\n",
    "dat['num_comments'] = num_comments\n",
    "dat['is_self'] = is_self\n",
    "dat['is_video'] = is_video\n",
    "dat['subreddit'] = subreddit\n",
    "dat = dat.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(dat)):\n",
    "    if ('\\\\' in dat['title'][i]):\n",
    "        print(i)\n",
    "        print(dat['title'][i])\n",
    "        \n",
    "'\\\\' in 'can it find \\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is wrangled into a format that is easier to use, it needs to be processed. It will be processed as follows:  \n",
    "  \n",
    "- Removal of stopwords\n",
    "- Tokenization\n",
    "- Lemmatization or stemming\n",
    "- Moving to a vector representation (bag of words, word2vec, or fasttext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the fasttext package direct from Facebook's Github will be employed, mostly because using it is pretty simple and does not really require the normal NLP preprocessing.  \n",
    "  \n",
    "It can be trained in supervised mode which supplies a classifier, though it requires the data be loaded from an external text file where each entry looks like this:  \n",
    "  \n",
    "\\__label__*thelabel* The text  \n",
    "  \n",
    "Where \"*thelabel*\" is the label of that data point and \"The text\" is the actual text. The part \"\\__label__\" needs to be  placed as is to mark what the label is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits into training and test data\n",
    "train = dat[0:int(len(dat)*.9)]\n",
    "test = dat[int(len(dat)*.9):len(dat)].reset_index(drop=True)\n",
    "datafile = []\n",
    "#Generates data file for output based on training data\n",
    "for i in range(len(train)):\n",
    "    datafile.append('__label__'+str(train['subreddit'][i])+' '+train['title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This saves the data as a text file as per fasttext's requirements\n",
    "#this opens the data file\n",
    "outfile = open('data.txt','w',errors='ignore')\n",
    "#This runs through the data and adds each line\n",
    "for line in datafile:\n",
    "    outfile.write(line)\n",
    "    outfile.write('\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains the fasttext model\n",
    "model = fasttext.train_supervised('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This runs all of the training and testing examples through the fasttext classifier\n",
    "trainout = []\n",
    "for i in range(len(train)):\n",
    "    trainout.append(int(model.predict(train['title'][i])[0][0][9]))\n",
    "testout = []\n",
    "for i in range(len(test)):\n",
    "    testout.append(int(model.predict(test['title'][i])[0][0][9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is: 0.6255826962520977\n",
      "The test set accuracy is: 0.5654362416107382\n"
     ]
    }
   ],
   "source": [
    "#This counts up the training and test error\n",
    "trainacc = 0\n",
    "#loops over training data\n",
    "for i in range(len(train)):\n",
    "    #If the model's output is different from the \n",
    "    if (train['subreddit'][i]!=trainout[i]):\n",
    "        trainacc = trainacc + 1\n",
    "trainacc = 1- trainacc/len(trainout)\n",
    "\n",
    "testacc = 0\n",
    "for i in range(len(test)):\n",
    "    if (test['subreddit'][i] != testout[i]):\n",
    "        testacc = testacc + 1\n",
    "testacc = 1-testacc/len(testout)\n",
    "print('The training set accuracy is: '+str(trainacc))\n",
    "print('The test set accuracy is: '+str(testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics askreddit worldnews funny gaming aww\n",
      "          0         1         2         3         4         5\n",
      "0  0.802198  0.010989  0.142857  0.021978  0.010989  0.010989\n",
      "1  0.000000  0.950980  0.009804  0.009804  0.000000  0.029412\n",
      "2  0.457143  0.019048  0.457143  0.038095  0.009524  0.019048\n",
      "3  0.029412  0.029412  0.019608  0.441176  0.098039  0.382353\n",
      "4  0.030303  0.090909  0.040404  0.414141  0.161616  0.262626\n",
      "5  0.030928  0.010309  0.010309  0.268041  0.082474  0.597938\n"
     ]
    }
   ],
   "source": [
    "#This uses sklearn to plot the confusion matrix for the test set, just to see how it looks\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('politics','askreddit','worldnews','funny','gaming','aww')\n",
    "confusion = pd.DataFrame(confusion_matrix(test['subreddit'],testout,normalize='true'))\n",
    "print(confusion)\n",
    "#predicted along x-axis, true along y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix actually shows some pretty interesting results. The highest accuracy rating goes to AskReddit where it correctly predicts it 95% of the time while also rarely falsely predicting askreddit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
