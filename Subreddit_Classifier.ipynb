{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classifier  \n",
    "  \n",
    "The social media platform Reddit is divided up into different subreddits. Each subreddit is a messageboard devoted to a certain topic. Each post contains a title and some combination of body text, images, videos, or links to external websites. People viewing the content can comment on it. Reddit uses a vote system where everybody can vote on each post or comment, either increasing the score (an \"upvote\") or decreasing the score (a \"downvote\") where comments or posts with higher scores generally being made more visible to anybody viewing the website. Some of the popular subreddit topics can include politics, news, humourous content, or AskReddit, a subreddit where people post questions and other people answer the questions in the comment section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape Reddit content, PRAW (the Python Reddit API Wrapper) was employed. It requires a Reddit developer account which gives credentials needed to access the API. Since I am only pulling content and not posting anything, I am accessing the API in read-only mode which requires fewer credentials than accessing it in read-write mode.  \n",
    "  \n",
    "The cell below handles imports and initializes PRAW with my credentials that are loaded via an external file. In order for this to work for you, you will need your own Reddit API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and setting up the reddit API\n",
    "import pandas as pd\n",
    "import os\n",
    "import praw\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "#Client ID, secret, user agent in that order\n",
    "credentials = list(pd.read_csv(os.getcwd()+'/credentials.csv').columns)\n",
    "\n",
    "reddit = praw.Reddit(client_id=credentials[0],\n",
    "                     client_secret=credentials[1],user_agent=credentials[2])\n",
    "\n",
    "#I want:\n",
    "#Title, text, link (if applicable), top comments, author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary test, the cell below gathers the top 1000 posts (highest score) from six subreddits. These subreddits are:  \n",
    "politics, a subreddit focused on US politics,  \n",
    "AskReddit, a subreddit where people post questions and others answer them in the comment section,  \n",
    "Worldnews, a subreddit focused on international, non-US news,  \n",
    "Funny, a subreddit mostly for memes and joke content,  \n",
    "Gaming, a subreddit dedicated to news or stories about videogames,  \n",
    "Aww, a subreddit dedicated to photos and videos of animals, mostly pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathers top posts from a few subreddits\n",
    "politics = list(reddit.subreddit('politics').top(limit=1000))\n",
    "ask = list(reddit.subreddit('askreddit').top(limit=1000))\n",
    "worldnews = list(reddit.subreddit('worldnews').top(limit=1000))\n",
    "funny = list(reddit.subreddit('funny').top(limit=1000))\n",
    "gaming = list(reddit.subreddit('gaming').top(limit=1000))\n",
    "aww = list(reddit.subreddit('aww').top(limit=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will gather lists of PRAW submission objects which then need to have the relevant data extracted from them.  \n",
    "My general idea is to create a Pandas dataframe containing all of the relevant data. Since PRAW requires each submission be handled individually (I cannot just grab the list, type list.title and get a list of all of the titles), I will be creating empty lists and appending the relevant values to them while looping over my lists of submissions.  \n",
    "  \n",
    "For now, Reddit comments will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing empty lists\n",
    "titles = []\n",
    "is_self = []\n",
    "is_video = []\n",
    "selftext = []\n",
    "author = []\n",
    "created = []\n",
    "num_comments = []\n",
    "score = []\n",
    "subreddit = []\n",
    "for i in range(len(politics)):\n",
    "    titles.append(politics[i].title)\n",
    "    is_self.append(politics[i].is_self)\n",
    "    is_video.append(politics[i].is_video)\n",
    "    selftext.append(politics[i].selftext)\n",
    "    author.append(politics[i].author)\n",
    "    created.append(politics[i].created)\n",
    "    num_comments.append(politics[i].num_comments)\n",
    "    score.append(politics[i].score)\n",
    "    subreddit.append(0)\n",
    "for i in range(len(ask)):\n",
    "    titles.append(ask[i].title)\n",
    "    is_self.append(ask[i].is_self)\n",
    "    is_video.append(ask[i].is_video)\n",
    "    selftext.append(ask[i].selftext)\n",
    "    author.append(ask[i].author)\n",
    "    created.append(ask[i].created)\n",
    "    num_comments.append(ask[i].num_comments)\n",
    "    score.append(ask[i].score)\n",
    "    subreddit.append(1)\n",
    "for i in range(len(worldnews)):\n",
    "    titles.append(worldnews[i].title)\n",
    "    is_self.append(worldnews[i].is_self)\n",
    "    is_video.append(worldnews[i].is_video)\n",
    "    selftext.append(worldnews[i].selftext)\n",
    "    author.append(worldnews[i].author)\n",
    "    created.append(worldnews[i].created)\n",
    "    num_comments.append(worldnews[i].num_comments)\n",
    "    score.append(worldnews[i].score)\n",
    "    subreddit.append(2)\n",
    "for i in range(len(funny)):\n",
    "    titles.append(funny[i].title)\n",
    "    is_self.append(funny[i].is_self)\n",
    "    is_video.append(funny[i].is_video)\n",
    "    selftext.append(funny[i].selftext)\n",
    "    author.append(funny[i].author)\n",
    "    created.append(funny[i].created)\n",
    "    num_comments.append(funny[i].num_comments)\n",
    "    score.append(funny[i].score)\n",
    "    subreddit.append(3)\n",
    "for i in range(len(gaming)):\n",
    "    titles.append(gaming[i].title)\n",
    "    is_self.append(gaming[i].is_self)\n",
    "    is_video.append(gaming[i].is_video)\n",
    "    selftext.append(gaming[i].selftext)\n",
    "    author.append(gaming[i].author)\n",
    "    created.append(gaming[i].created)\n",
    "    num_comments.append(gaming[i].num_comments)\n",
    "    score.append(gaming[i].score)\n",
    "    subreddit.append(4)\n",
    "for i in range(len(aww)):\n",
    "    titles.append(aww[i].title)\n",
    "    is_self.append(aww[i].is_self)\n",
    "    is_video.append(aww[i].is_video)\n",
    "    selftext.append(aww[i].selftext)\n",
    "    author.append(aww[i].author)\n",
    "    created.append(aww[i].created)\n",
    "    num_comments.append(aww[i].num_comments)\n",
    "    score.append(aww[i].score)\n",
    "    subreddit.append(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a Pandas dataframe to hold the data\n",
    "dat = pd.DataFrame()\n",
    "dat['title'] = titles\n",
    "dat['text'] = selftext\n",
    "dat['author'] = author\n",
    "dat['created'] = created\n",
    "dat['score'] = score\n",
    "dat['num_comments'] = num_comments\n",
    "dat['is_self'] = is_self\n",
    "dat['is_video'] = is_video\n",
    "dat['subreddit'] = subreddit\n",
    "dat = dat.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 titles removed\n"
     ]
    }
   ],
   "source": [
    "#Here, short titles are dropped since they are unlikely to have much predictive power \n",
    "from nltk.tokenize import word_tokenize\n",
    "short_count = 0\n",
    "for i in range(len(dat)):\n",
    "    if (len(word_tokenize(dat['title'][i]))<2):\n",
    "        short_count = short_count + 1\n",
    "        dat = dat.drop(i,axis='index')\n",
    "print(str(short_count)+' titles removed')\n",
    "#Resets the index of the dataframe due to the dropped frames\n",
    "dat = dat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is wrangled into a format that is easier to use, I am just going to shove it into the fasttext library Facebook keeps on their research Github. It might not work that well, but it'll at least be a quick prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the fasttext package direct from Facebook's Github will be employed, mostly because using it is pretty simple and does not really require the normal NLP preprocessing.  \n",
    "  \n",
    "It can be trained in supervised mode which supplies a classifier, though it requires the data be loaded from an external text file where each entry looks like this:  \n",
    "  \n",
    "\\__label__*thelabel* The text  \n",
    "  \n",
    "Where \"*thelabel*\" is the label of that data point and \"The text\" is the actual text. The part \"\\__label__\" needs to be  placed as is to mark what the label is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits into training and test data\n",
    "train = dat[0:int(len(dat)*.9)]\n",
    "test = dat[int(len(dat)*.9):len(dat)].reset_index(drop=True)\n",
    "datafile = []\n",
    "#Generates data file for output based on training data\n",
    "for i in range(len(train)):\n",
    "    datafile.append('__label__'+str(train['subreddit'][i])+' '+train['title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This saves the data as a text file as per fasttext's requirements\n",
    "#this opens the data file\n",
    "outfile = open('data.txt','w',errors='ignore')\n",
    "#This runs through the data and adds each line\n",
    "for line in datafile:\n",
    "    outfile.write(line)\n",
    "    outfile.write('\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains the fasttext model\n",
    "model = fasttext.train_supervised('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This runs all of the training and testing examples through the fasttext classifier\n",
    "trainout = []\n",
    "for i in range(len(train)):\n",
    "    trainout.append(int(model.predict(train['title'][i])[0][0][9]))\n",
    "testout = []\n",
    "for i in range(len(test)):\n",
    "    testout.append(int(model.predict(test['title'][i])[0][0][9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is: 0.6309166040570999\n",
      "The test set accuracy is: 0.5591216216216216\n"
     ]
    }
   ],
   "source": [
    "#This counts up the training and test error\n",
    "trainacc = 0\n",
    "#loops over training data\n",
    "for i in range(len(train)):\n",
    "    #If the model's output is different from the \n",
    "    if (train['subreddit'][i]!=trainout[i]):\n",
    "        trainacc = trainacc + 1\n",
    "trainacc = 1- trainacc/len(trainout)\n",
    "\n",
    "testacc = 0\n",
    "for i in range(len(test)):\n",
    "    if (test['subreddit'][i] != testout[i]):\n",
    "        testacc = testacc + 1\n",
    "testacc = 1-testacc/len(testout)\n",
    "print('The training set accuracy is: '+str(trainacc))\n",
    "print('The test set accuracy is: '+str(testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics askreddit worldnews funny gaming aww\n",
      "          0         1         2         3         4         5\n",
      "0  0.835165  0.010989  0.109890  0.010989  0.021978  0.010989\n",
      "1  0.000000  0.950617  0.000000  0.000000  0.024691  0.024691\n",
      "2  0.436893  0.038835  0.495146  0.029126  0.000000  0.000000\n",
      "3  0.030928  0.020619  0.051546  0.371134  0.113402  0.412371\n",
      "4  0.009346  0.028037  0.028037  0.401869  0.214953  0.317757\n",
      "5  0.008850  0.017699  0.017699  0.274336  0.079646  0.601770\n"
     ]
    }
   ],
   "source": [
    "#This uses sklearn to plot the confusion matrix for the test set, just to see how it looks\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('politics','askreddit','worldnews','funny','gaming','aww')\n",
    "confusion = pd.DataFrame(confusion_matrix(test['subreddit'],testout,normalize='true'))\n",
    "print(confusion)\n",
    "#predicted along x-axis, true along y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix actually shows some pretty interesting results. The highest accuracy rating goes to AskReddit where it correctly predicts it 95% of the time while also rarely falsely predicting askreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work very well. Maybe it's the classifier, instead of using the supervised fasttext model, I will try the unsupervised fasttext embeddings and then use boosting. I had a hard time finding a pretrained Fasttext model, so here I train it myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "dataset = api.load('text8')\n",
    "c = list(dataset)\n",
    "from gensim.models import FastText\n",
    "unsupftt = FastText(size=40)\n",
    "unsupftt.build_vocab(c)\n",
    "\n",
    "#model = fasttext.load_model(os.getcwd()+'/fasttextmodel.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupftt.train(c,total_examples=unsupftt.corpus_count,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "embeddings = np.zeros((40,len(dat)))\n",
    "for i in range(len(dat)):\n",
    "    tokenized = word_tokenize(dat['title'][i])\n",
    "    avg = np.zeros((1,40))\n",
    "    for j in range(len(tokenized)):\n",
    "        avg = avg + unsupftt.wv.get_vector(tokenized[j])\n",
    "    avg = avg/len(tokenized)\n",
    "    embeddings[:,i] = avg\n",
    "    \n",
    "embeddings = embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=3, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xg\n",
    "trainx = embeddings[0:5500]\n",
    "testx = embeddings[5501:]\n",
    "trainy = dat['subreddit'][0:5500]\n",
    "testy = dat['subreddit'][5501:]\n",
    "model = xg.XGBClassifier(gamma=3,subsample=0.8)\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9232727272727272\n",
      "Test set accuracy:\n",
      "0.491566265060241\n",
      "          0         1         2         3         4         5\n",
      "0  0.560606  0.030303  0.378788  0.000000  0.030303  0.000000\n",
      "1  0.000000  0.820000  0.060000  0.040000  0.040000  0.040000\n",
      "2  0.202703  0.054054  0.621622  0.027027  0.054054  0.040541\n",
      "3  0.041096  0.150685  0.041096  0.301370  0.246575  0.219178\n",
      "4  0.123288  0.123288  0.109589  0.178082  0.260274  0.205479\n",
      "5  0.025316  0.126582  0.063291  0.202532  0.088608  0.493671\n"
     ]
    }
   ],
   "source": [
    "#With the trained model, show accuracy and the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tfidftrain = model.predict(trainx)\n",
    "tfidfout = model.predict(testx)\n",
    "confusion2 = pd.DataFrame(confusion_matrix(testy,tfidfout,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==tfidftrain)/len(tfidftrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==tfidfout)/len(tfidfout))\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "FastText didn't seem to work that well, though the data up until now has barely been cleaned or processed, which is typically important in natural language applications. I will start with the following: \n",
    "  \n",
    "- Word Tokenization (splitting the strings into a list of one-word strings)\n",
    "- Removal of stopwords\n",
    "- Lemmatization or stemming\n",
    "- Moving to a vector representation (bag of words, word2vec, or fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "titles = []\n",
    "#This tokenizes, removes stopwords and stems\n",
    "for i in range(len(dat)):\n",
    "    #tokenizes by word\n",
    "    tmp_title = word_tokenize(dat['title'][i].lower())\n",
    "    #removes stop words\n",
    "    tmp_title2 = [i for i in tmp_title if i not in stop]\n",
    "    #stems the words\n",
    "    tmp_title3 = [ps.stem(i) for i in tmp_title2]\n",
    "    titles.append(tmp_title3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn's TF-IDF vectorizer actually seems to do a lot of the work for you, so I'll give that a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=((1,3)))\n",
    "x = vectorizer.fit_transform(dat['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.3, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xg\n",
    "#Split into training and test data\n",
    "trainx = x[0:5500]\n",
    "testx = x[5501:]\n",
    "trainy = dat['subreddit'][0:5500]\n",
    "testy = dat['subreddit'][5501:]\n",
    "#Instantiate model with given hyperparameters\n",
    "model = xg.XGBClassifier(gamma=0.3,subsample=0.8)\n",
    "#Train model\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.914\n",
      "Test set accuracy:\n",
      "0.655421686746988\n",
      "          0         1         2         3         4         5\n",
      "0  0.772727  0.015152  0.181818  0.015152  0.015152  0.000000\n",
      "1  0.020000  0.960000  0.000000  0.020000  0.000000  0.000000\n",
      "2  0.216216  0.040541  0.662162  0.027027  0.027027  0.027027\n",
      "3  0.027397  0.013699  0.068493  0.547945  0.150685  0.191781\n",
      "4  0.027397  0.041096  0.027397  0.356164  0.479452  0.068493\n",
      "5  0.000000  0.000000  0.012658  0.253165  0.113924  0.620253\n"
     ]
    }
   ],
   "source": [
    "#With the trained model, show accuracy and the confusion matrix\n",
    "#Get output values\n",
    "tfidftrain = model.predict(trainx)\n",
    "tfidfout = model.predict(testx)\n",
    "#Puts the confusion matrix in a Pandas dataframe so that it prints nicer\n",
    "confusion2 = pd.DataFrame(confusion_matrix(testy,tfidfout,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==tfidftrain)/len(tfidftrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==tfidfout)/len(tfidfout))\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is a little surprising, tf-idf actually seems to work better than FastText here, both in having a greater overall test set accuracy and in resolving some of the issues with the confusion matrix. Its ability to tell apart the subreddits \"funny\" and \"gaming,\" while still not stellar, has notably improved from FastText. This could be partly because the titles are typically quite short which would make word embeddings lose some of their power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9821818181818182\n",
      "Test set accuracy:\n",
      "0.653012048192771\n",
      "          0         1         2         3         4         5\n",
      "0  0.818182  0.045455  0.121212  0.000000  0.015152  0.000000\n",
      "1  0.020000  0.980000  0.000000  0.000000  0.000000  0.000000\n",
      "2  0.216216  0.094595  0.662162  0.013514  0.000000  0.013514\n",
      "3  0.054795  0.246575  0.082192  0.301370  0.136986  0.178082\n",
      "4  0.068493  0.219178  0.000000  0.082192  0.452055  0.178082\n",
      "5  0.000000  0.113924  0.050633  0.000000  0.025316  0.810127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#This data is continuous so Gaussian naive Bayes may make more sense, but sparse data (like output from TF-IDF)\n",
    "#is unlikely to fit a Gaussian distribution, so even though multinomial naive Bayes is more suited to categorical\n",
    "#data it is still likely the better choice.\n",
    "\n",
    "#Create multinomial naive Bayes object\n",
    "MnNB = MultinomialNB(fit_prior=True)\n",
    "#Fit naive Bayes\n",
    "MnNB.fit(trainx,trainy)\n",
    "#Get output from model on test and training sets\n",
    "MnNBtrain = MnNB.predict(trainx)\n",
    "MnNBtest = MnNB.predict(testx)\n",
    "confusion3 = pd.DataFrame(confusion_matrix(testy,MnNBtest,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==MnNBtrain)/len(MnNBtrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==MnNBtest)/len(MnNBtest))\n",
    "print(confusion3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrices for XGBoost vs Naive Bayes, they both have distinct strengths and weaknesses. Naive Bayes seems to do better for the classes that are not 3 and 4, whereas XGBoost does a better job of separating 3 and 4. Somehow combining the two classifiers might work well, but Naive Bayes generally seems biased against predicting 3 and 4, especially 3 which it predicts very infrequently. It also predicts the \"more accurate\" classes a lot more often, with classes 1 and 5 being predicted far more often than they actually appear in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what was seen above, with tf-idf performing better than Fasttext, it seems likely that Fasttext was poorly trained. The model was trained with only 40 dimensions due to constraints on available computing power, a Fasttext model with higher dimensionality may perform better. Typically, 300 dimensions is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, I could not find a pretrained fasttext model, so I decided to try a pretrained glove model instead. Gensim actually has one of those available which is loaded in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 111] Connection refused>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1317\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1318\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1251\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    965\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    937\u001b[0m         self.sock = self._create_connection(\n\u001b[0;32m--> 938\u001b[0;31m             (self.host,self.port), self.timeout, self.source_address)\n\u001b[0m\u001b[1;32m    939\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetsockopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIPPROTO_TCP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTCP_NODELAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-54b89a921c59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove-twitter-25'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jason/.local/lib/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jason/.local/lib/python3.7/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0mtmp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0minit_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__init__.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_load_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0mtotal_parts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_parts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1360\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 111] Connection refused>"
     ]
    }
   ],
   "source": [
    "glove = api.load('glove-twitter-25')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
