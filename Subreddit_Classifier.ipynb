{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subreddit Classifier  \n",
    "  \n",
    "The social media platform Reddit is divided up into different subreddits. Each subreddit is a messageboard devoted to a certain topic. Each post contains a title and some combination of body text, images, videos, or links to external websites. People viewing the content can comment on it. Reddit uses a vote system where everybody can vote on each post or comment, either increasing the score (an \"upvote\") or decreasing the score (a \"downvote\") where comments or posts with higher scores generally being made more visible to anybody viewing the website. Some of the popular subreddit topics can include politics, news, humourous content, or AskReddit, a subreddit where people post questions and other people answer the questions in the comment section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to scrape Reddit content, PRAW (the Python Reddit API Wrapper) was employed. It requires a Reddit developer account which gives credentials needed to access the API. Since I am only pulling content and not posting anything, I am accessing the API in read-only mode which requires fewer credentials than accessing it in read-write mode.  \n",
    "  \n",
    "The cell below handles imports and initializes PRAW with my credentials that are loaded via an external file. In order for this to work for you, you will need your own Reddit API credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and setting up the reddit API\n",
    "import pandas as pd\n",
    "import os\n",
    "import praw\n",
    "import numpy as np\n",
    "import fasttext\n",
    "\n",
    "#Client ID, secret, user agent in that order\n",
    "credentials = list(pd.read_csv(os.getcwd()+'/credentials.csv').columns)\n",
    "\n",
    "reddit = praw.Reddit(client_id=credentials[0],\n",
    "                     client_secret=credentials[1],user_agent=credentials[2])\n",
    "\n",
    "#I want:\n",
    "#Title, text, link (if applicable), top comments, author\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preliminary test, the cell below gathers the top 1000 posts (highest score) from six subreddits. These subreddits are:  \n",
    "politics, a subreddit focused on US politics,  \n",
    "AskReddit, a subreddit where people post questions and others answer them in the comment section,  \n",
    "Worldnews, a subreddit focused on international, non-US news,  \n",
    "Funny, a subreddit mostly for memes and joke content,  \n",
    "Gaming, a subreddit dedicated to news or stories about videogames,  \n",
    "Aww, a subreddit dedicated to photos and videos of animals, mostly pets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gathers top posts from a few subreddits\n",
    "politics = list(reddit.subreddit('politics').top(limit=1000))\n",
    "ask = list(reddit.subreddit('askreddit').top(limit=1000))\n",
    "worldnews = list(reddit.subreddit('worldnews').top(limit=1000))\n",
    "funny = list(reddit.subreddit('funny').top(limit=1000))\n",
    "gaming = list(reddit.subreddit('gaming').top(limit=1000))\n",
    "aww = list(reddit.subreddit('aww').top(limit=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above will gather lists of PRAW submission objects which then need to have the relevant data extracted from them.  \n",
    "My general idea is to create a Pandas dataframe containing all of the relevant data. Since PRAW requires each submission be handled individually (I cannot just grab the list, type list.title and get a list of all of the titles), I will be creating empty lists and appending the relevant values to them while looping over my lists of submissions.  \n",
    "  \n",
    "For now, Reddit comments will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing empty lists\n",
    "titles = []\n",
    "is_self = []\n",
    "is_video = []\n",
    "selftext = []\n",
    "author = []\n",
    "created = []\n",
    "num_comments = []\n",
    "score = []\n",
    "subreddit = []\n",
    "for i in range(len(politics)):\n",
    "    titles.append(politics[i].title)\n",
    "    is_self.append(politics[i].is_self)\n",
    "    is_video.append(politics[i].is_video)\n",
    "    selftext.append(politics[i].selftext)\n",
    "    author.append(politics[i].author)\n",
    "    created.append(politics[i].created)\n",
    "    num_comments.append(politics[i].num_comments)\n",
    "    score.append(politics[i].score)\n",
    "    subreddit.append(0)\n",
    "for i in range(len(ask)):\n",
    "    titles.append(ask[i].title)\n",
    "    is_self.append(ask[i].is_self)\n",
    "    is_video.append(ask[i].is_video)\n",
    "    selftext.append(ask[i].selftext)\n",
    "    author.append(ask[i].author)\n",
    "    created.append(ask[i].created)\n",
    "    num_comments.append(ask[i].num_comments)\n",
    "    score.append(ask[i].score)\n",
    "    subreddit.append(1)\n",
    "for i in range(len(worldnews)):\n",
    "    titles.append(worldnews[i].title)\n",
    "    is_self.append(worldnews[i].is_self)\n",
    "    is_video.append(worldnews[i].is_video)\n",
    "    selftext.append(worldnews[i].selftext)\n",
    "    author.append(worldnews[i].author)\n",
    "    created.append(worldnews[i].created)\n",
    "    num_comments.append(worldnews[i].num_comments)\n",
    "    score.append(worldnews[i].score)\n",
    "    subreddit.append(2)\n",
    "for i in range(len(funny)):\n",
    "    titles.append(funny[i].title)\n",
    "    is_self.append(funny[i].is_self)\n",
    "    is_video.append(funny[i].is_video)\n",
    "    selftext.append(funny[i].selftext)\n",
    "    author.append(funny[i].author)\n",
    "    created.append(funny[i].created)\n",
    "    num_comments.append(funny[i].num_comments)\n",
    "    score.append(funny[i].score)\n",
    "    subreddit.append(3)\n",
    "for i in range(len(gaming)):\n",
    "    titles.append(gaming[i].title)\n",
    "    is_self.append(gaming[i].is_self)\n",
    "    is_video.append(gaming[i].is_video)\n",
    "    selftext.append(gaming[i].selftext)\n",
    "    author.append(gaming[i].author)\n",
    "    created.append(gaming[i].created)\n",
    "    num_comments.append(gaming[i].num_comments)\n",
    "    score.append(gaming[i].score)\n",
    "    subreddit.append(4)\n",
    "for i in range(len(aww)):\n",
    "    titles.append(aww[i].title)\n",
    "    is_self.append(aww[i].is_self)\n",
    "    is_video.append(aww[i].is_video)\n",
    "    selftext.append(aww[i].selftext)\n",
    "    author.append(aww[i].author)\n",
    "    created.append(aww[i].created)\n",
    "    num_comments.append(aww[i].num_comments)\n",
    "    score.append(aww[i].score)\n",
    "    subreddit.append(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a Pandas dataframe to hold the data\n",
    "dat = pd.DataFrame()\n",
    "dat['title'] = titles\n",
    "dat['text'] = selftext\n",
    "dat['author'] = author\n",
    "dat['created'] = created\n",
    "dat['score'] = score\n",
    "dat['num_comments'] = num_comments\n",
    "dat['is_self'] = is_self\n",
    "dat['is_video'] = is_video\n",
    "dat['subreddit'] = subreddit\n",
    "dat = dat.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 titles removed\n"
     ]
    }
   ],
   "source": [
    "#Here, short titles are dropped since they are unlikely to have much predictive power \n",
    "from nltk.tokenize import word_tokenize\n",
    "short_count = 0\n",
    "for i in range(len(dat)):\n",
    "    if (len(word_tokenize(dat['title'][i]))<2):\n",
    "        short_count = short_count + 1\n",
    "        dat = dat.drop(i,axis='index')\n",
    "print(str(short_count)+' titles removed')\n",
    "#Resets the index of the dataframe due to the dropped frames\n",
    "dat = dat.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is wrangled into a format that is easier to use, I am just going to shove it into the fasttext library Facebook keeps on their research Github. It might not work that well, but it'll at least be a quick prototype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the fasttext package direct from Facebook's Github will be employed, mostly because using it is pretty simple and does not really require the normal NLP preprocessing.  \n",
    "  \n",
    "It can be trained in supervised mode which supplies a classifier, though it requires the data be loaded from an external text file where each entry looks like this:  \n",
    "  \n",
    "\\__label__*thelabel* The text  \n",
    "  \n",
    "Where \"*thelabel*\" is the label of that data point and \"The text\" is the actual text. The part \"\\__label__\" needs to be  placed as is to mark what the label is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits into training and test data\n",
    "train = dat[0:int(len(dat)*.9)]\n",
    "test = dat[int(len(dat)*.9):len(dat)].reset_index(drop=True)\n",
    "datafile = []\n",
    "#Generates data file for output based on training data\n",
    "for i in range(len(train)):\n",
    "    datafile.append('__label__'+str(train['subreddit'][i])+' '+train['title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This saves the data as a text file as per fasttext's requirements\n",
    "#this opens the data file\n",
    "outfile = open('data.txt','w',errors='ignore')\n",
    "#This runs through the data and adds each line\n",
    "for line in datafile:\n",
    "    outfile.write(line)\n",
    "    outfile.write('\\n')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains the fasttext model\n",
    "model = fasttext.train_supervised('data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This runs all of the training and testing examples through the fasttext classifier\n",
    "trainout = []\n",
    "for i in range(len(train)):\n",
    "    trainout.append(int(model.predict(train['title'][i])[0][0][9]))\n",
    "testout = []\n",
    "for i in range(len(test)):\n",
    "    testout.append(int(model.predict(test['title'][i])[0][0][9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training set accuracy is: 0.6162659654395192\n",
      "The test set accuracy is: 0.5692567567567568\n"
     ]
    }
   ],
   "source": [
    "#This counts up the training and test error\n",
    "trainacc = 0\n",
    "#loops over training data\n",
    "for i in range(len(train)):\n",
    "    #If the model's output is different from the \n",
    "    if (train['subreddit'][i]!=trainout[i]):\n",
    "        trainacc = trainacc + 1\n",
    "trainacc = 1- trainacc/len(trainout)\n",
    "\n",
    "testacc = 0\n",
    "for i in range(len(test)):\n",
    "    if (test['subreddit'][i] != testout[i]):\n",
    "        testacc = testacc + 1\n",
    "testacc = 1-testacc/len(testout)\n",
    "print('The training set accuracy is: '+str(trainacc))\n",
    "print('The test set accuracy is: '+str(testacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics askreddit worldnews funny gaming aww\n",
      "          0         1         2         3         4         5\n",
      "0  0.825243  0.009709  0.106796  0.029126  0.009709  0.019417\n",
      "1  0.000000  0.950000  0.040000  0.000000  0.010000  0.000000\n",
      "2  0.442308  0.019231  0.451923  0.038462  0.028846  0.019231\n",
      "3  0.030928  0.020619  0.072165  0.185567  0.247423  0.443299\n",
      "4  0.019417  0.038835  0.048544  0.203883  0.339806  0.349515\n",
      "5  0.000000  0.023529  0.000000  0.211765  0.094118  0.670588\n"
     ]
    }
   ],
   "source": [
    "#This uses sklearn to plot the confusion matrix for the test set, just to see how it looks\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('politics','askreddit','worldnews','funny','gaming','aww')\n",
    "confusion = pd.DataFrame(confusion_matrix(test['subreddit'],testout,normalize='true'))\n",
    "print(confusion)\n",
    "#predicted along x-axis, true along y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix actually shows some pretty interesting results. The highest accuracy rating goes to AskReddit where it correctly predicts it 95% of the time while also rarely falsely predicting askreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didn't work very well. Maybe it's the classifier, instead of using the supervised fasttext model, I will try the unsupervised fasttext embeddings and then use boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "dataset = api.load('text8')\n",
    "c = list(dataset)\n",
    "from gensim.models import FastText\n",
    "unsupftt = FastText(size=40)\n",
    "unsupftt.build_vocab(c)\n",
    "\n",
    "#model = fasttext.load_model(os.getcwd()+'/fasttextmodel.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupftt.train(c,total_examples=unsupftt.corpus_count,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "embeddings = np.zeros((40,len(dat)))\n",
    "for i in range(len(dat)):\n",
    "    tokenized = word_tokenize(dat['title'][i])\n",
    "    avg = np.zeros((1,40))\n",
    "    for j in range(len(tokenized)):\n",
    "        avg = avg + unsupftt.wv.get_vector(tokenized[j])\n",
    "    avg = avg/len(tokenized)\n",
    "    embeddings[:,i] = avg\n",
    "    \n",
    "embeddings = embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=3, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xg\n",
    "trainx = embeddings[0:5500]\n",
    "testx = embeddings[5501:]\n",
    "trainy = dat['subreddit'][0:5500]\n",
    "testy = dat['subreddit'][5501:]\n",
    "model = xg.XGBClassifier(gamma=3,subsample=0.8)\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9312727272727273\n",
      "Test set accuracy:\n",
      "0.4819277108433735\n",
      "          0         1         2         3         4         5\n",
      "0  0.552632  0.065789  0.302632  0.000000  0.039474  0.039474\n",
      "1  0.028571  0.728571  0.100000  0.028571  0.071429  0.042857\n",
      "2  0.250000  0.041667  0.625000  0.055556  0.013889  0.013889\n",
      "3  0.057143  0.057143  0.071429  0.242857  0.257143  0.314286\n",
      "4  0.130435  0.115942  0.115942  0.130435  0.289855  0.217391\n",
      "5  0.017241  0.155172  0.051724  0.224138  0.120690  0.431034\n"
     ]
    }
   ],
   "source": [
    "#With the trained model, show accuracy and the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tfidftrain = model.predict(trainx)\n",
    "tfidfout = model.predict(testx)\n",
    "confusion2 = pd.DataFrame(confusion_matrix(testy,tfidfout,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==tfidftrain)/len(tfidftrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==tfidfout)/len(tfidfout))\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "FastText didn't seem to work that well, though the data up until now has barely been cleaned or processed, which is typically important in natural language applications. I will start with the following: \n",
    "  \n",
    "- Word Tokenization (splitting the strings into a list of one-word strings)\n",
    "- Removal of stopwords\n",
    "- Lemmatization or stemming\n",
    "- Moving to a vector representation (bag of words, word2vec, or fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "titles = []\n",
    "#This tokenizes, removes stopwords and stems\n",
    "for i in range(len(dat)):\n",
    "    #tokenizes by word\n",
    "    tmp_title = word_tokenize(dat['title'][i].lower())\n",
    "    #removes stop words\n",
    "    tmp_title2 = [i for i in tmp_title if i not in stop]\n",
    "    #stems the words\n",
    "    tmp_title3 = [ps.stem(i) for i in tmp_title2]\n",
    "    titles.append(tmp_title3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn's TF-IDF vectorizer actually seems to do a lot of the work for you, so I'll give that a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=((1,3)))\n",
    "x = vectorizer.fit_transform(dat['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0.3, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=0.8,\n",
       "              tree_method=None, validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xg\n",
    "#Split into training and test data\n",
    "trainx = x[0:5500]\n",
    "testx = x[5501:]\n",
    "trainy = dat['subreddit'][0:5500]\n",
    "testy = dat['subreddit'][5501:]\n",
    "#Instantiate model with given hyperparameters\n",
    "model = xg.XGBClassifier(gamma=0.3,subsample=0.8)\n",
    "#Train model\n",
    "model.fit(trainx,trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9132727272727272\n",
      "Test set accuracy:\n",
      "0.6602409638554216\n",
      "          0         1         2         3         4         5\n",
      "0  0.805195  0.000000  0.129870  0.051948  0.000000  0.012987\n",
      "1  0.000000  0.971014  0.000000  0.014493  0.014493  0.000000\n",
      "2  0.152778  0.013889  0.777778  0.027778  0.027778  0.000000\n",
      "3  0.000000  0.000000  0.058824  0.367647  0.235294  0.338235\n",
      "4  0.014493  0.000000  0.057971  0.304348  0.478261  0.144928\n",
      "5  0.000000  0.000000  0.016667  0.316667  0.150000  0.516667\n"
     ]
    }
   ],
   "source": [
    "#With the trained model, show accuracy and the confusion matrix\n",
    "#Get output values\n",
    "tfidftrain = model.predict(trainx)\n",
    "tfidfout = model.predict(testx)\n",
    "#Puts the confusion matrix in a Pandas dataframe so that it prints nicer\n",
    "confusion2 = pd.DataFrame(confusion_matrix(testy,tfidfout,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==tfidftrain)/len(tfidftrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==tfidfout)/len(tfidfout))\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is a little surprising, tf-idf actually seems to work better than FastText here, both in having a greater overall test set accuracy and in resolving some of the issues with the confusion matrix. Its ability to tell apart the subreddits \"funny\" and \"gaming,\" while still not stellar, has notably improved from FastText. This could be partly because the titles are typically quite short which would make word embeddings lose some of their power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:\n",
      "0.9843636363636363\n",
      "Test set accuracy:\n",
      "0.6698795180722892\n",
      "          0         1         2         3         4         5\n",
      "0  0.740260  0.064935  0.168831  0.012987  0.000000  0.012987\n",
      "1  0.000000  0.971014  0.014493  0.000000  0.000000  0.014493\n",
      "2  0.083333  0.041667  0.847222  0.000000  0.000000  0.027778\n",
      "3  0.029412  0.264706  0.044118  0.132353  0.044118  0.485294\n",
      "4  0.028986  0.086957  0.000000  0.086957  0.550725  0.246377\n",
      "5  0.000000  0.116667  0.000000  0.100000  0.016667  0.766667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#This data is continuous so Gaussian naive Bayes may make more sense, but sparse data (like output from TF-IDF)\n",
    "#is unlikely to fit a Gaussian distribution, so even though multinomial naive Bayes is more suited to categorical\n",
    "#data it is still likely the better choice.\n",
    "\n",
    "#Create multinomial naive Bayes object\n",
    "MnNB = MultinomialNB(fit_prior=True)\n",
    "#Fit naive Bayes\n",
    "MnNB.fit(trainx,trainy)\n",
    "#Get output from model on test and training sets\n",
    "MnNBtrain = MnNB.predict(trainx)\n",
    "MnNBtest = MnNB.predict(testx)\n",
    "confusion3 = pd.DataFrame(confusion_matrix(testy,MnNBtest,normalize='true'))\n",
    "print('Training set accuracy:')\n",
    "print(sum(trainy==MnNBtrain)/len(MnNBtrain))\n",
    "print('Test set accuracy:')\n",
    "print(sum(testy==MnNBtest)/len(MnNBtest))\n",
    "print(confusion3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the confusion matrices for XGBoost vs Naive Bayes, they both have distinct strengths and weaknesses. Naive Bayes seems to do better for the classes that are not 3 and 4, whereas XGBoost does a better job of separating 3 and 4. Somehow combining the two classifiers might work well, but Naive Bayes generally seems biased against predicting 3 and 4, especially 3 which it predicts very infrequently. It also predicts the \"more accurate\" classes a lot more often, with classes 1 and 5 being predicted far more often than they actually appear in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
